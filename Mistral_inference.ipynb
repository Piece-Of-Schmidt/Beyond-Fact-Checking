{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init mistral client\n",
    "client = MistralClient(\n",
    "    api_key='<paste_mistral_key_here>',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_csv('data/prompts.csv') # read settings\n",
    "articles = pd.read_csv('data/evaluation_texts.csv') # read articles\n",
    "criteria = pd.read_csv('data/criteria.csv') # read criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define iteration variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = prompts['key'].tolist()\n",
    "criteria = criteria['kriterien'].tolist()\n",
    "models = ['open-mistral-7b', 'open-mixtral-8x22b', 'mistral-large-latest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select subset for demonstration purposes\n",
    "models = ['open-mistral-7b']\n",
    "articles = articles.iloc[0:2]\n",
    "techniques = techniques[0:2]\n",
    "criteria = criteria[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    print('\\n#### - MODEL: ', model, ' - ####\\n')\n",
    "    \n",
    "    # init list for model \n",
    "    results_per_model = {}\n",
    "\n",
    "    # iterate over all techniques\n",
    "    for technic in techniques:\n",
    "\n",
    "        print('\\nNEXT PROMPT STYLE:', technic, '\\n')\n",
    "\n",
    "        # get criteria right\n",
    "        krits = criteria if 'each' in technic else ['\\n'.join(f'{i+1} {criteria[i]}' for i in range(len(criteria)))]\n",
    "\n",
    "        # get instruction right\n",
    "        instruction = prompts[prompts['key'] == technic]['value'].tolist()\n",
    "\n",
    "        # init predictions_list that entails all articles\n",
    "        predictions = {}\n",
    "\n",
    "        # iterate over all documents\n",
    "        for row in articles.itertuples():\n",
    "\n",
    "            print('  article:', row.id)\n",
    "            \n",
    "            # init prediction_per_crit list that entails the predictions for one single document\n",
    "            prediction_per_crit = {}\n",
    "\n",
    "            # iterate over all criteria\n",
    "            for idx, krit in enumerate(krits):\n",
    "\n",
    "                print('   crit:', krit)\n",
    "\n",
    "                # create complete promt\n",
    "                prompt = f'{instruction}\\n\\n#ARTIKEL:\\n{text}\\n\\n#KRIT.:\\n{krit}'\n",
    "\n",
    "                # mimic three runs (groq does not offer a parameter 'n')\n",
    "                runs = []\n",
    "                for i in [0,1,2]:\n",
    "\n",
    "                    chat_completion = client.chat(\n",
    "                        messages=[\n",
    "                            ChatMessage(role='system', content='Du bist strenger wissenschaftlicher Gutachter von wissenschaftsjournalistischen Texten.'),\n",
    "                            ChatMessage(role='user', content=prompt),\n",
    "                        ],\n",
    "                        temperature=0.1,\n",
    "                        model=model,\n",
    "                    )\n",
    "\n",
    "                    runs.append(chat_completion.choices[0].message.content)\n",
    "\n",
    "                # append predictions_per_crit list (one article - all predictions)\n",
    "                prediction_per_crit['prompt'+str(idx)] = runs\n",
    "\n",
    "            # append predictions list (all articles - same prompting)\n",
    "            predictions[row.id] = prediction_per_crit\n",
    "\n",
    "        # append results_per_model list - data from all runs for selected model\n",
    "        results_per_model[technic] = predictions\n",
    "\n",
    "    # append all_results list - all data\n",
    "    all_results[model] = results_per_model\n",
    "\n",
    "# return\n",
    "# all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "with open('Mistral_evaluations.json', 'w', encoding='utf-8') as outfile: \n",
    "    json.dump(all_results, outfile, indent=5, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
